{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning deep neural networks Ôºç MTCNN \n",
    "\n",
    "Large models are memory-intensive with millions of parameters. Moving around all of the data required to compute inference results consumes energy. Many of the layers are bandwidth-bound, which means that the execution latency is dominated by the available bandwidth. The storage and transfer of large neural networks is also a challenge. Network pruning can reduce the footprint of a neural network, increase its inference speed and save energy, reduce the amount of bandwidth and compute required. A related idea motivating pruning is that models are over-parametrized and contain redundant logic and features which don't contribute a lot to the output.\n",
    "\n",
    "There are different types of sparsity patterns, ranging from irregular to regular as shown below. The simplest case is the element-wise sparity --- **fine-grained pruning**. The use of specialized hardware to see a performance gain from fine-grained weights sparsity is needed. In that case, the tensors are produced as sparse at the element granularity. The weight tensors are not reduced in size since the zero-coefficients are still present. Some NN accelerators (ASICs) take advantage of fine-grained sparsity by using a compressed representation of sparse tensors. \n",
    "\n",
    "Coarse-grained pruning referred to as **structured pruning, group pruning or block pruning**. Structured-pruning such as Channel and filter pruning create compressed models that do not require special hardware to execute. This makes this form of structured pruning particularly interesting and popular. Convolution weights are 4D:(F, C, K, K) where F is the number of filters, C is the number of channels, and K is the kernel size. A kernal is a 2D matrix (K, K) that is a part of a 3D filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/1.png\"  width=\"600\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent works advocate \"Structured sparsity\" where entire conv2d filters are pruned. if less filters operating on a certain layer, less output feature maps will be.  In the most commom scenario, besides we reconfigure the convolution layer by changing the \"out_channels\"and the corresponding channels of linked activation layer or batch normalization layer, the following convolution layer \"in_channels\" needs to be changed. The following layer's weights need to be shrinked by removing the in_channels corresponding to the filters we prunned. If the following layer is a fully connected layer, the corresponding neurons will be discarded. The recent observation shows that the deeper the layer, the more it will get pruned.  \n",
    "\n",
    "It is a **data-dependency** type of prunning. The most state of art DNNs use more complicated structures such as resnet, mobilenet, inception layer. The prunning strategy will be quite different. I will explain how to prune the mobilefacenet in a separate tutorial. So far, understanding the above prunning logic is enough for MTCNN prunning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/2.png\"  width=\"400\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity Definition \n",
    "\n",
    "Sprasity is a measure of how many elements in a tensor are exact zeros. The L0 **norm** function measures how many zero-elements are in a tensor x. In other words, an element contributes either a value of 1 or 0 to L0. Anything but an exact zero contributies a value of 1.\n",
    "\n",
    "<img src=\"imgs/3.png\"  width=\"250\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Schedule\n",
    "\n",
    "The most straight-forward to prune is to take a trained model and prune it once; also called one-shot pruning. However, employing a pruning-followed-by-retraining regimen can achieve much better results (higher sparsity at no accuracy loss). This is called **iterative pruning**, and the retraining that follows pruning is often referred as **fine-tuning**.  The iterative pruning can be considered as repeatedly learning which weights are important, removing the least important ones and then retraining the model to let it \"recover\" from the prunning by adjusting the remaining weights. At each iteration, we prune more weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/4.png\"  width=\"200\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Criteria\n",
    "\n",
    "Pruning requires a criteria for choosing which elements / kernals / filters to prune - this is called the pruning criteria. The most common criteria is the L1 norm of the weights of each filter. For each pruning iteration, all the filters are ranked, the m lowest ranking filters are prunned, retrain and repeat. The more sophisticated ranking approach is to rank the filters based on the effect of each on the network cost. That is, the network cost change will be minimal when pruning them. The ranking method is based on a first order of taylor expansion of the network cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/5.png\"  width=\"500\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta is the ranking score that we care. hi refers to a filter to be pruned. C is the total cost. The ranking of filter **h** becomes the abs of first order of taylor expansion on the derivative of cost on corresponding feature map. For example if the feature map (activation) is in shape of (32x256x112x112) - (batch_size x channel x kernal x kernal), the corresponding gradient of cost will be the same shape. The point wise multiplication of each activation in the batch and it's gradient is averaged except the dimension of output leads to a 256 sized vector representing the ranks of the 256 filters in this layer. The ranking of each layer are then normalized by the L2 norm of the ranks in that layer which is believed as a empiric behavior   \n",
    "\n",
    "The whole idea is from [Nvidia](https://arxiv.org/abs/1611.06440). In the paper their method outperformed other methods in accuracy. \n",
    "\n",
    "The following section will step-by-step explain how to determine the right filters to be pruned and how to filter the model. You may want to refer the detailed codes for a complete iterative prunning process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Filters to be Prunned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rock on the MTCNN Prunning. Import the MTCNN Network and take a peek on MTCNN model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from MTCNN.Base_Model.MTCNN_nets import PNet, RNet, ONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNet(\n",
      "  (features): Sequential(\n",
      "    (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu1): PReLU(num_parameters=10)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=16)\n",
      "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu3): PReLU(num_parameters=32)\n",
      "  )\n",
      "  (conv4_1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv4_2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "RNet(\n",
      "  (features): Sequential(\n",
      "    (conv1): Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu1): PReLU(num_parameters=28)\n",
      "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv2): Conv2d(28, 48, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=48)\n",
      "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv3): Conv2d(48, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (prelu3): PReLU(num_parameters=64)\n",
      "    (flatten): Flatten()\n",
      "    (conv4): Linear(in_features=576, out_features=128, bias=True)\n",
      "    (prelu4): PReLU(num_parameters=128)\n",
      "  )\n",
      "  (conv5_1): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (conv5_2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "ONet(\n",
      "  (features): Sequential(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu1): PReLU(num_parameters=32)\n",
      "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=64)\n",
      "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu3): PReLU(num_parameters=64)\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (prelu4): PReLU(num_parameters=128)\n",
      "    (flatten): Flatten()\n",
      "    (conv5): Linear(in_features=1152, out_features=256, bias=True)\n",
      "    (drop5): Dropout(p=0.25)\n",
      "    (prelu5): PReLU(num_parameters=256)\n",
      "  )\n",
      "  (conv6_1): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (conv6_2): Linear(in_features=256, out_features=4, bias=True)\n",
      "  (conv6_3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pnet = PNet(is_train=True).to(device)\n",
    "pnet.load_state_dict(torch.load(\"MTCNN/Base_Model/pnet_Weights\", map_location=lambda storage, loc: storage))\n",
    "rnet = RNet(is_train=True).to(device)\n",
    "rnet.load_state_dict(torch.load(\"MTCNN/Base_Model/rnet_Weights\", map_location=lambda storage, loc: storage))\n",
    "onet = ONet(is_train=True).to(device)\n",
    "onet.load_state_dict(torch.load(\"MTCNN/Base_Model/onet_Weights\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "pnet.train()\n",
    "rnet.train()\n",
    "onet.train()\n",
    "print(pnet)\n",
    "print(rnet)\n",
    "print(onet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure is to look over all the feature maps derived from conv2d layers, calculate the first order of taylor expansion, rank all the conv2d filters and prune the m lowest ranking filters. \n",
    "\n",
    "Ideally, the filter ranking should be concluded by using all the training data. For demonstrated purpose, we only use one single fake batch data here including input images, ground truth label and ground truth offset for bounding boxes. We will use PNet as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = torch.randn(3,3,12,12)\n",
    "gt_label = torch.Tensor([1,0,-1]).type(torch.LongTensor)\n",
    "gt_offset = torch.randn(3,4).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the grade of the intermedia feature map on the total loss, we hereby use **register_hook** function. \n",
    "\n",
    "A **FilterPrunner** Class is built as below:\n",
    "\n",
    "1. The **forward** function will append the generated intermedia feature maps from conv2d layer to activations dict. \n",
    "\n",
    "2. The **compute_rank** function will pointwisely multiply the grad with activation and average the tayor value for each filter, build up a filter ranks dict during loss backward calculation \n",
    "\n",
    "3. **normalize_ranks_per_layer** to normalize the taylor value of each filter in that layer\n",
    "\n",
    "4. **lowest_ranking_filters** to use \"nsmallest\" function for filter ranking\n",
    "\n",
    "5. **get_prunning_plan** to obtain the layer index and filter index that will be prunned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "from operator import itemgetter\n",
    "from heapq import nsmallest\n",
    "\n",
    "class FilterPrunner:\n",
    "    def __init__(self, model, use_cuda = False):\n",
    "        self.model = model\n",
    "        self.reset()\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "    def reset(self):\n",
    "        self.filter_ranks = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "        self.grad_index = 0\n",
    "        self.activation_to_layer = {}\n",
    "\n",
    "        activation_index = 0\n",
    "        for layer, (name, module) in enumerate(self.model.features._modules.items()):\n",
    "            x = module(x)\n",
    "            if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "                x.register_hook(self.compute_rank)\n",
    "                self.activations.append(x)\n",
    "                self.activation_to_layer[activation_index] = layer # the ith conv2d layer\n",
    "                activation_index += 1\n",
    "                \n",
    "        a = self.model.conv4_1(x)\n",
    "        b = self.model.conv4_2(x)\n",
    "        c = None \n",
    "\n",
    "        return c, b, a\n",
    "\n",
    "    def compute_rank(self, grad):\n",
    "        activation_index = len(self.activations) - self.grad_index - 1\n",
    "        activation = self.activations[activation_index]\n",
    "        print(\"activation shape is: \", activation.shape)\n",
    "        taylor = activation * grad\n",
    "\n",
    "        # Get the average value for every filter,\n",
    "        # accross all the other dimensions\n",
    "        taylor = taylor.mean(dim=(0, 2, 3)).data\n",
    "        print(\"taylor shape is: \", taylor.shape)\n",
    "        if activation_index not in self.filter_ranks:\n",
    "            self.filter_ranks[activation_index] = \\\n",
    "                torch.FloatTensor(activation.size(1)).zero_()\n",
    "\n",
    "            if self.use_cuda:\n",
    "                self.filter_ranks[activation_index] = self.filter_ranks[activation_index].cuda()\n",
    "\n",
    "        self.filter_ranks[activation_index] += taylor\n",
    "        self.grad_index += 1\n",
    "        \n",
    "    def lowest_ranking_filters(self, num):\n",
    "        data = []\n",
    "        for i in sorted(self.filter_ranks.keys()):\n",
    "            for j in range(self.filter_ranks[i].size(0)):\n",
    "                data.append((self.activation_to_layer[i], j, self.filter_ranks[i][j]))\n",
    "\n",
    "        return nsmallest(num, data, itemgetter(2))\n",
    "\n",
    "    def normalize_ranks_per_layer(self):\n",
    "        for i in self.filter_ranks:\n",
    "            v = torch.abs(self.filter_ranks[i]).cpu()\n",
    "            v = v / np.sqrt(torch.sum(v * v))\n",
    "            self.filter_ranks[i] = v\n",
    "\n",
    "    def get_prunning_plan(self, num_filters_to_prune):\n",
    "        filters_to_prune = self.lowest_ranking_filters(num_filters_to_prune)\n",
    "                \n",
    "        filters_to_prune_per_layer = {}\n",
    "        for (l, f, _) in filters_to_prune:\n",
    "            if l not in filters_to_prune_per_layer:\n",
    "                filters_to_prune_per_layer[l] = []\n",
    "            filters_to_prune_per_layer[l].append(f)\n",
    "    \n",
    "    \n",
    "        for l in filters_to_prune_per_layer:\n",
    "            filters_to_prune_per_layer[l] = sorted(filters_to_prune_per_layer[l])\n",
    "\n",
    "        return filters_to_prune_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to calculate the loss, carry out backpropagation to obtain grad and build up the filter_ranks. Note that if the whole training data is used for filter ranking, the taylor value for each filter will be accumulated through batches. The ranking could be more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation shape is:  torch.Size([3, 32, 1, 1])\n",
      "taylor shape is:  torch.Size([32])\n",
      "activation shape is:  torch.Size([3, 16, 3, 3])\n",
      "taylor shape is:  torch.Size([16])\n",
      "activation shape is:  torch.Size([3, 10, 10, 10])\n",
      "taylor shape is:  torch.Size([10])\n",
      "2 torch.Size([32])\n",
      "1 torch.Size([16])\n",
      "0 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "prunner = FilterPrunner(pnet)\n",
    "prunner.reset()\n",
    "\n",
    "loss_cls = nn.CrossEntropyLoss()\n",
    "loss_offset = nn.MSELoss()\n",
    "\n",
    "pnet.zero_grad()\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "        _, pred_offsets, pred_label = prunner.forward(input_images)\n",
    "        pred_offsets = torch.squeeze(pred_offsets)\n",
    "        pred_label = torch.squeeze(pred_label)\n",
    "        # calculate the cls loss\n",
    "        # get the mask element which >= 0, only 0 and 1 can effect the detection loss\n",
    "        mask_cls = torch.ge(gt_label, 0)\n",
    "        valid_gt_label = gt_label[mask_cls]\n",
    "        valid_pred_label = pred_label[mask_cls]\n",
    "\n",
    "        # calculate the box loss\n",
    "        # get the mask element which != 0\n",
    "        unmask = torch.eq(gt_label, 0)\n",
    "        mask_offset = torch.eq(unmask, 0)\n",
    "        valid_gt_offset = gt_offset[mask_offset]\n",
    "        valid_pred_offset = pred_offsets[mask_offset]\n",
    "\n",
    "        loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "        if len(valid_gt_label) != 0:\n",
    "            loss += 0.02*loss_cls(valid_pred_label, valid_gt_label)\n",
    "\n",
    "        if len(valid_gt_offset) != 0:\n",
    "            loss += 0.6*loss_offset(valid_pred_offset, valid_gt_offset)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "for i , taylor in prunner.filter_ranks.items():\n",
    "    print(i, taylor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Normalize the taylor value by the L2 norm of the ranks in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunner.normalize_ranks_per_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we would like to prune 10 filters. **lowest_ranking_filters** will rank the filter based on the taylor value and return the 10 smallest filters in the format of (layer_index, filter_index, taylor value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 21, tensor(0.0002)),\n",
       " (5, 20, tensor(0.0003)),\n",
       " (5, 1, tensor(0.0003)),\n",
       " (5, 16, tensor(0.0003)),\n",
       " (5, 0, tensor(0.0009)),\n",
       " (3, 8, tensor(0.0009)),\n",
       " (5, 8, tensor(0.0010)),\n",
       " (5, 18, tensor(0.0012)),\n",
       " (5, 29, tensor(0.0025)),\n",
       " (5, 11, tensor(0.0028))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_filters_to_prune = 10\n",
    "filters_to_prune = prunner.lowest_ranking_filters(num_filters_to_prune) \n",
    "filters_to_prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_prunning_plan** will return a dict such that key: layer index, values: filter index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: [0, 1, 8, 11, 16, 18, 20, 21, 29], 3: [8]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters_to_prune_per_layer = prunner.get_prunning_plan(num_filters_to_prune)\n",
    "filters_to_prune_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Prune the MTCNN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already find out the layer index and filter index with smallest taylor values. These filters are what we would like to take out from Network. But how to prune the network ? Let's move on \n",
    "\n",
    "There are two scenarios on prunning MTCNN net. Once a certain conv2d layer with certian filters is choosen to be pruned, we will first prune the corresponding channels on conv2d, bounded activation layer (i.e. PReLU) and batch normalization layer. There is no need for pooling layer. If (A) the next linked layer is a Conv2d layer, we need to reconfige the in-channels of this layer. if (B) the next linked layer is a fully connected layer, the corresponding input neurons will need to be discarded. \n",
    "\n",
    "Let's use onet as an example since onet has both conv layer and linear layer. Assume we want to prune layer 0 with filter 3 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index = 0\n",
    "filter_index = (3,6)\n",
    "\n",
    "_, conv = list(onet.features._modules.items())[layer_index]\n",
    "_, PReLU = list(onet.features._modules.items())[layer_index+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new conv and PReLu can be constructed by removing the corresponding filter weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv old_weight shape is:  (32, 3, 3, 3)\n",
      "conv new_weight shape is:  (30, 3, 3, 3)\n",
      "conv old_bias shape is:  (32,)\n",
      "conv new_bias shape is:  (30,)\n",
      "PReLU old_weight's shape is:  (32,)\n",
      "PReLU new_weight's shape is:  (30,)\n"
     ]
    }
   ],
   "source": [
    "new_conv = \\\n",
    "        torch.nn.Conv2d(in_channels=conv.in_channels, \\\n",
    "                        out_channels=conv.out_channels - len(filter_index),\n",
    "                        kernel_size=conv.kernel_size, \\\n",
    "                        stride=conv.stride,\n",
    "                        padding=conv.padding,\n",
    "                        dilation=conv.dilation,\n",
    "                        groups=conv.groups,\n",
    "                        bias=(conv.bias is not None))\n",
    "\n",
    "old_weights = conv.weight.data.cpu().numpy()  \n",
    "print(\"conv old_weight shape is: \", old_weights.shape)\n",
    "new_weights = np.delete(old_weights, filter_index, axis=0)  \n",
    "new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "print(\"conv new_weight shape is: \", new_weights.shape)\n",
    "\n",
    "bias_numpy = conv.bias.data.cpu().numpy()\n",
    "print(\"conv old_bias shape is: \", bias_numpy.shape)\n",
    "bias = np.delete(bias_numpy, filter_index)\n",
    "new_conv.bias.data = torch.from_numpy(bias)\n",
    "print(\"conv new_bias shape is: \", bias.shape)\n",
    "\n",
    "# The new PReLU layer constructed as follow:    \n",
    "new_PReLU = torch.nn.PReLU(num_parameters=PReLU.num_parameters-len(filter_index))\n",
    "old_weights = PReLU.weight.data.cpu().numpy()\n",
    "print(\"PReLU old_weight's shape is: \", old_weights.shape)\n",
    "new_weights = np.delete(old_weights, filter_index)\n",
    "new_PReLU.weight.data = torch.from_numpy(new_weights)\n",
    "print(\"PReLU new_weight's shape is: \", new_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the next conv to be pruned. The in_channels need to be reconstructed and corresponding weights will be removed. Note that the linked activation layer or batch normalization layer do not need to be edited since the output channels are not changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "next_conv = None\n",
    "offset = 1\n",
    "while layer_index + offset < len(onet.features._modules.items()):\n",
    "    res = list(onet.features._modules.items())[layer_index + offset]\n",
    "    if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
    "        next_name, next_conv = res\n",
    "        break\n",
    "    offset = offset + 1\n",
    "\n",
    "print(next_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv old_weight shape is:  (64, 32, 3, 3)\n",
      "conv new_weight shape is:  (64, 30, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "if not next_conv is None:\n",
    "    next_new_conv = \\\n",
    "        torch.nn.Conv2d(in_channels=next_conv.in_channels - len(filter_index), \\\n",
    "                        out_channels=next_conv.out_channels, \\\n",
    "                        kernel_size=next_conv.kernel_size, \\\n",
    "                        stride=next_conv.stride,\n",
    "                        padding=next_conv.padding,\n",
    "                        dilation=next_conv.dilation,\n",
    "                        groups=next_conv.groups,\n",
    "                        bias=(next_conv.bias is not None))\n",
    "\n",
    "    old_weights = next_conv.weight.data.cpu().numpy() \n",
    "    print(\"conv old_weight shape is: \", old_weights.shape)\n",
    "    new_weights = np.delete(old_weights, filter_index, axis=1)  \n",
    "    next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "    print(\"conv new_weight shape is: \", new_weights.shape)\n",
    "\n",
    "    next_new_conv.bias.data = next_conv.bias.data  # bias is not changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the layers with new constructed ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_layers(model, i, indexes, layers):\n",
    "\n",
    "    \"\"\"\n",
    "    replace conv layers of model.feature\n",
    "\n",
    "    :param model:\n",
    "    :param i: index of model.feature\n",
    "    :param indexes: array of indexes of layers to be replaced\n",
    "    :param layers: array of new layers to replace\n",
    "    :return: model with replaced layers\n",
    "    \"\"\"\n",
    "    if i in indexes:\n",
    "        return layers[indexes.index(i)]\n",
    "    return model[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONet(\n",
       "  (conv6_1): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (conv6_2): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (conv6_3): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): PReLU(num_parameters=30)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Conv2d(30, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): PReLU(num_parameters=64)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): PReLU(num_parameters=64)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (10): PReLU(num_parameters=128)\n",
       "    (11): Flatten()\n",
       "    (12): Linear(in_features=1152, out_features=256, bias=True)\n",
       "    (13): Dropout(p=0.25)\n",
       "    (14): PReLU(num_parameters=256)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.nn.Sequential(\n",
    "    *(replace_layers(onet.features, i, [layer_index, layer_index+1, layer_index + offset], \\\n",
    "                     [new_conv, new_PReLU, next_new_conv]) for i, _ in enumerate(onet.features)))\n",
    "del onet.features  # reset\n",
    "del conv # reset\n",
    "\n",
    "onet.features = features\n",
    "onet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the next linked layer is a fully connected layer, the corresponding input neurons will need to be discarded. Assume we want to prune the conv2d layer index 9 with filter index 4, 30. \n",
    "\n",
    "We will omit the conv2d layer reconstruction process here but only the fully connected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "Linear(in_features=1152, out_features=256, bias=True)\n"
     ]
    }
   ],
   "source": [
    "layer_index = 9\n",
    "filter_index = (4,30)\n",
    "\n",
    "_, conv = list(onet.features._modules.items())[layer_index]\n",
    "\n",
    "linear_layer = None\n",
    "offset = 1\n",
    "while layer_index + offset < len(onet.features._modules.items()):\n",
    "    res = list(onet.features._modules.items())[layer_index + offset]\n",
    "    if isinstance(res[1], torch.nn.Linear):\n",
    "        layer_name, linear_layer = res\n",
    "        break\n",
    "    offset = offset + 1\n",
    "\n",
    "print(conv)\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parames per input channel is calculated from the upper conv out channels.\n",
    "The corresponding to-be-deleted neuron is derived based on filter index and params per input channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear old weights shape is:  (256, 1152)\n",
      "linear new weights shape is:  (256, 1134)\n"
     ]
    }
   ],
   "source": [
    "params_per_input_channel = linear_layer.in_features // conv.out_channels\n",
    "\n",
    "new_linear_layer = torch.nn.Linear(linear_layer.in_features - len(filter_index)*params_per_input_channel,linear_layer.out_features)\n",
    "\n",
    "old_weights = linear_layer.weight.data.cpu().numpy()  #i.e. (out_feature x in_feature)\n",
    "print('linear old weights shape is: ', old_weights.shape)\n",
    "\n",
    "delete_array = []\n",
    "for filter in filter_index:\n",
    "    delete_array += [filter * params_per_input_channel + x for x in range(params_per_input_channel)]\n",
    "    new_weights = np.delete(old_weights, delete_array, axis=1)  \n",
    "print('linear new weights shape is: ', new_weights.shape)\n",
    "new_linear_layer.bias.data = linear_layer.bias.data\n",
    "new_linear_layer.weight.data = torch.from_numpy(new_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
